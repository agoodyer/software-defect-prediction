\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{tabularx}
\usepackage{caption}
\usepackage{float}
\usepackage{hyperref}
\usepackage{soul,xcolor}
\usepackage{cancel}
\usepackage{amsmath}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Software Defect Prediction: \\ \large \normalfont An exploration of Model Interpretability in Software Defect Analysis }



\author{Aidan Goodyer, Mason Azzopardi \\
  \texttt{\{goodyera,azzoparm\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}


In Software Engineering, static code metrics are often used as targets to define a 'quality' gate for a given module or piece of code.  
However, investing time into ensuring these metrics are met is painstaking, and many seem to be poor measures of the module's correctness. 

\noindent A well-known critique of metric-driven evaluation comes from economist Charles Goodhart, whose law states that: 

\begin{quote}
"When a measure becomes a target, it ceases to be a good measure". 
\end{quote}

  \noindent We seek to investigate the legitimacy of this statement as it pertains to static code metrics. Rather than aiming to create the best possible classifier for software defects, we instead aim to answer the question:

\begin{quote}
  "Which static software metrics \textit{actually} matter"?
\end{quote}


  \noindent To investigate this question, we adopt an approach centered on feature selection and model interpretability. 
  Specifically, we employ logistic regression in conjunction with L1 regularization to favour sparsity in the learned parameter, allowing us to derive an understanding of which metrics contribute meaningfully to defect prediction. 
  By favouring an interpretable sparse model, we aim to provide analytical insight into the value of these metrics.



\section{Related Work}

Here, talk about the related work you encountered for your approach. Cite at least 5 references. Refer to item 2. No one has done exactly your task? Write about the most similar thing you can find. This should be around 0.25-0.5 pages.

\section{Dataset}


To explore the robustness of static metrics as defect predictors, we select two datasets to investigate. By training on two independent sources, we aim to verify that our results generalize across different software engineering domains, or otherwise identify how specific software paradigms or development environments influence the efficacy of defect discovery. 

\subsection{NASA Promise Dataset}

The NASA Promise Dataset is a collection of roughly 10,000 examples in software defect analysis. These examples consist of static code measures collected from NASA's Metrics Data Program (MDP). Data was collected from real software modules used by various NASA projects over a span of several decades. Data is partitioned into 13 different subsets according to the separate software project source and derived measures, which vary between sets. The attributes are primarily Halstead, McCabe, and Lines of Code related metrics.  



\noindent The original PROMISE repository being no longer accessible, thereforew we opted to use a GitHub mirror of the original dataset. It can be located \underline{\href{https://github.com/klainfo/NASADefectDataset}{here.}}

\subsection{GHRP Dataset}

The second dataset used is the baseline from the \href{https://github.com/feiwww/GHPR_dataset}{Github Pull Request} (GHRP) dataset. It was created using the \href{https://github.com/mauricioaniche/ckURL}{CK} code metrics tool on the 6052 instances of bug related pull requests. It contains the SHA to the sample code instance, 21 static code metrics, and a label (1/0) informing whether there is a defect or not for every instance.

\subsection{Preprocessing}

The original \textbf{PROMISE} dataset ($D'$) contains significant flaws including duplicate entries, empty modules, and inconsistencies that may pose challenges in training an effective classifier. As such, we opt to start from a pre-cleaned version of the dataset ($D''$) to reduce manual cleaning and deduplication efforts. $D''$ is frequently used as a common baseline across several of the referenced papers.To preprocess the dataset, we convert the source \verb|.arff| files to  \verb|.csv| using a minimal python script.

Since the numerical ranges vary extremely for individual input features, we utilize z-score normalization to standardize the deign matrix to have a mean of 0 and a standard deviation of 1 using the formula: 

$$ \phi(x) = \frac{x-\mu}{\sigma}$$

where:

$$ \mu = \frac{1}{n} \sum_{i=1}^n x_i \hspace{5mm} \sigma = \sqrt{ \frac{1}{n}\sum_{i=1}^n (x_i-\mu)^2} $$

Defining a consistent scale is particularly important for the purpose of comparing feature importance, as this ensures that features cannot be scaled by an arbitrary constant to affect the magnitude of its learned weight. 




The \textbf{GHPR} dataset required little preprocessing, as its baseline included far less data duplication and errors than the PROMISE dataset. 
\begin{enumerate}
  \item SHA column removed
  \item label seperated from static code metrics and assigned as the labels and input values respectively
  \item the labels and input values were split into an 80/20 train/test split
  \item input values are standardized
  \item \textbf{FCNN ONLY} input values transformed into tensor values
\end{enumerate}





\section{Features}

The \textbf{PROMISE} dataset is comprised of 21 static code metrics along with a binary Y/N label indicating the presence of defects. We plan to use all of the included metrics in our model, except for 'LOC\_\allowbreak BLANK', 'LOC\_\allowbreak TOTAL', and 'LOC\_\allowbreak AND\_\allowbreak COMMENT'. The rationale for dropping these metrics is due to their extreme collineariy, as these metrics are simply proxies for Lines of Code in every case. 

We can formulate the model as a binary classification problem of the form: 

\begin{itemize}
  \item \textbf{Input Feature (x):} The 21 static code metrics.
  \item \textbf{Label (y):} Binary Defect / No-Defect Label.
\end{itemize}

See Table \underline{\ref{tab:PROMISE metrics}} for a description of each of the 21 metrics. 


\begin{table}[htbp]
    \centering
    \footnotesize 
    \begin{tabularx}{\columnwidth}{@{} l X @{}}
        \toprule
        \textbf{Metric} & \textbf{Description} \\
        \midrule
        1. loc (v) & Line count of code \\
        2. v (g) & Cyclomatic complexity \\
        3. ev (g) & Essential complexity \\
        4. iv (g) & Design complexity \\
        5. \textcolor{red}{\st{loCode}} & Line count \\
        6. loComment & Count of lines of comments \\
        7. \textcolor{red}{\st{loBlank}} & Count of blank lines \\
        8. \textcolor{red}{\st{loCodeAndComment}} & Count of code and comment lines \\
        9. uniq\_Op & Unique operators \\
        10. uniq\_Opnd & Unique operands \\
        11. total\_Op & Total operators \\
        12. total\_Opnd & Total operands \\
        13. branchCount & Branch count of the flow graphs \\
        14. n & Total operators + operands \\
        15. v & Volume \\
        16. l & Program length \\
        17. d & Difficulty \\
        18. i & Intelligence \\
        19. e & Effort \\
        20. b & Estimate of the effort \\
        21. t & Time estimator \\
        22. Defect & True/False \\
        \bottomrule
    \end{tabularx}
        \caption{The summary of PROMISE code metrics (red unused)\label{tab:PROMISE metrics}}
\end{table}


The \textbf{GHPR} dataset is comprised of 21 static code metrics along with a binary Y/N label indicating the presence of defects. We plan to use all of the included metrics in our model.
\begin{table}[htbp]
    \centering
    \footnotesize 
    \begin{tabularx}{\columnwidth}{@{} l X @{}}
        \toprule
        \textbf{Metric} & \textbf{Description} \\
        \midrule
        1. CBO (v) & Coupling between objects. Counts the number of dependencies a class has. \\
        2. WMC (g) & 	Weight Method Class or McCabe's complexity. It counts the number of branch instructions in a class.\\
        3. DIT (g) & Depth Inheritance Tree. It counts the number of "fathers" a class has. All classes have DIT at least 1 (everyone inherits java.lang.Object). \\
        4. rfc (g) & Response for a Class. Counts the number of unique method invocations in a class. \\
        5. lcom & Lack of Cohesion of Methods. Calculates LCOM metric. \\
        6. totalMethods & Counts the number of methods. \\
        7. totalFields & Counts the number of fields. \\
        8. NOSI & Number of static invocations. Counts the number of invocations to static methods. \\
        9. LOC & Lines of code. It counts the lines of count, ignoring empty lines. \\
        10. returnQty & Quantity of returns. The number of return instructions. \\
        11. loopQty & Quantity of loops. The number of loops (i.e., for, while, do while, enhanced for). \\
        12. comparisonsQty & Quantity of comparisons. The number of comparisons (i.e., == and !=). \\
        13. tryCatchQty & Quantity of try/catches. The number of try/catches. \\
        14. parenthesizedExpsQty & Quantity of parenthesized expressions. The number of expressions inside parenthesis. \\
        15. stringLiteralsQty & String literals. The number of string literals (e.g., "John Doe"). \\
        16. numbersQty & Quantity of Number. The number of numbers (i.e., int, long, double, float) literals. \\
        17. assignmentsQty & Quantity of Variables. Number of declared variables. \\
        18. mathOperationsQty & Quantity of Math Operations: The number of math operations (times, divide, remainder, plus, minus, left shit, right shift). \\
        19. variablesQty & Quantity of Variables. Number of declared variables. \\
        20. maxNestedBlocks & Max nested blocks. The highest number of blocks nested together. \\
        21. uniqueWordsQty & Number of unique words. Number of unique words in the source code. \\
        \bottomrule
    \end{tabularx}
        \caption{The summary of GHPR code metrics\label{tab:GHRP metrics}}
\end{table}
We can formulate the model as a binary classification problem of the form: 

\begin{itemize}
  \item \textbf{Input Feature (x):} The 21 static code metrics.
  \item \textbf{Label (y):} Binary Defect / No-Defect Label.
\end{itemize}

See Table \underline{\ref{tab:metrics}} for a description of each of the 21 metrics. 

\section{Implementation}

\subsection{Logistic Regression Model}

Our implementation utilizes a Logistic Regression model trained using gradient descent. The model is modified to use L1 regularization and an asymmetric loss function to meet the needs of the defect prediction task. 

The probability of the prensence of a defect in a module, $p(i)$ is calculated using sigmoid activation:

$$
p(i) = \sigma\!\left( \theta^\top x^{(i)} + \theta_0 \right)
= \frac{1}{1 + e^{-\left( \theta^\top x^{(i)} + \theta_0 \right)}}
$$

Two key changes were made to favour interpretability and correct classification of the minority class. 

First, we modify the logistic regression to be asymmetric. When true defects are misclassified, we apply an asymmetric penalty to heavily discourage this type of mistake in the model. Since the consequences of missing a software defect are far more severe than investigating a few 'false alarm' cases, skewing the penalty favors a model that minimizes these critical mistakes. 

We define the weighted error term $e^{(i)}$ be: 


$$e^{(i)} = \begin{cases} 4(p^{(i)} - 1) & \text{if } y^{(i)} = 1 \\ p^{(i)} - 0 & \text{if } y^{(i)} = 0 \end{cases}$$


The second modification we make to standard logistic regression is L1 regularization. This choice encourages sparsity in the learned weights, helping to drive irrelevant metrics to zero within the model. This is ideal for our feature selection goals as it drives the model to surface the most 'useful' features for prediction. 

With this modification, the gradients of our loss function become: 

$$\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^{(t)}, \theta_0^{(t)}) = \frac{1}{n} \sum_{i=1}^{n} e^{(i)} \mathbf{x}^{(i)} + \lambda \cdot \text{sign}(\boldsymbol{\theta}^{(t)})$$

$$\nabla_{\theta_0}J(\boldsymbol{\theta}^{(t)}, \theta_0^{(t)}) = \frac{1}{n} \sum_{i=1}^{n} e^{(i)} $$


Using the definition of the $L_1$ norm:
$$\lVert x \rVert _1 = \sum_{i=1}^{n} |x_i|$$

\noindent The gradient of the $\lambda$ term becomes $\text{sign}(\boldsymbol{\theta}^{(t)})$ since $\frac{d}{dx} \left|x\right| = \text{sign}(x) $ 


Lastly, we use the standard gradient descent update rule (with $\eta = 0.1$): 


$$\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \eta \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^{(t)}, \theta_0^{(t)})$$

$$\theta_0^{(t+1)} = \theta_0^{(t)} - \eta \nabla_{\theta_0}J(\boldsymbol{\theta}^{(t)}, \theta_0^{(t)})$$

One additional modification we make prior to training is to log-transform our dataset by: 

$$x^{(i)}_{log} = ln(1+x^{(i)})$$

This step is important for improving the ability of our linear model to discern patterns at different scales, since many of the metrics included follow a power law relationship. By addressing the skeweness of the data we can compress the distribution and help the model perform better in the general case rather than latching on to outliers. This step is done prior to z-score normalization.

\noindent Our model is trained for 1000 iterations as experimentally convergence occurs within a safe margin of this bound. 



\subsection{Fully Connected Neural Network (FCNN)}
The FCNN model was constructed in PyTorch with the following architecture:
\begin{itemize}
    \item \textbf{Input Layer:} Accepts 21 input features
    \item \textbf{Hidden Layer 1:} 32 neurons, ReLU activation, 50\% Dropout.
    \item \textbf{Hidden Layer 2:} 16 neurons, ReLU activation, 50\% Dropout.
    \item \textbf{Output Layer:} Single neuron with sigmoid activation for classification
\end{itemize}
The FCNN model was trained with stochastic gradient descent (SGD) using a Binary Cross-Entropy Loss as its loss function.

BCE loss:
$$-{(y\log(p) + (1 - y)\log(1 - p))}$$

\section{Results and Evaluation}

\subsection{PROMISE Dataset Logistic Regression Model}

The logistic regression model was evaluated using an 80\% / 20\% train/test split. This split was done using the 'stratify' option to preserve the relative frequency of the minority class (defects) across both training and testing. This is important due to the substantial class imbalance, where a vast majority of the dataset is defect-free.   

\noindent \textbf{Confusion Matrix: Training Set}
\[
\begin{array}{c|cc}
 & \hat{y}=0 & \hat{y}=1 \\
\hline
y=0 & 3186~(\mathrm{TN}) & 1700~(\mathrm{FP}) \\
y=1 & 485~(\mathrm{FN}) & 805~(\mathrm{TP})
\end{array}
\]




\noindent \textbf{Confusion Matrix: Test Set}
\[
\begin{array}{c|cc}
 & \hat{y}=0 & \hat{y}=1 \\
\hline
y=0 & 787~(\mathrm{TN}) & 435~(\mathrm{FP}) \\
y=1 & 110~(\mathrm{FN}) & 212~(\mathrm{TP})
\end{array}
\]

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{../log-regression-figures/accuracy.png}
  \caption{Accuracy results for the Logistic Regression Model on the PROMISE dataset. Note that overall accuracy is low partially due to the intentional permissiveness of false positives. }
  \label{fig:accuracy-log-promise}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{../log-regression-figures/features.png}
  \caption{Feature Importance for the Logistic Regression Model on the PROMISE dataset}
  \label{fig:accuracy-log-promise}
\end{figure}

\begin{table*}[t] % The asterisk allows it to span both columns
    \centering
    \caption{Feature Importance for Defect Prediction (PROMISE Dataset)}
    \label{tab:feature_importance_halstead_wide}
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} lcl @{}}
        \toprule
        \textbf{Metric Name} & \textbf{Weight Magnitude} & \textbf{Impact} \\
        \midrule
        NUM\_UNIQUE\_OPERANDS & 0.2456 & $\uparrow$ Probability \\
        LOC\_EXECUTABLE & 0.1657 & $\uparrow$ Probability \\
        LOC\_COMMENTS & 0.0970 & $\uparrow$ Probability \\
        DESIGN\_COMPLEXITY & 0.0709 & $\uparrow$ Probability \\
        CYCLOMATIC\_COMPLEXITY & 0.0679 & $\uparrow$ Probability \\
        HALSTEAD\_ERROR\_EST & 0.0393 & $\uparrow$ Probability \\
        NUM\_OPERANDS & 0.0013 & Little Impact \\
        HALSTEAD\_EFFORT & 0.0012 & Little Impact \\
        HALSTEAD\_LEVEL & 0.0011 & Little Impact \\
        HALSTEAD\_PROG\_TIME & 0.0009 & Little Impact \\
        ESSENTIAL\_COMPLEXITY & 0.0008 & Little Impact \\
        HALSTEAD\_LENGTH & 0.0007 & Little Impact \\
        NUM\_UNIQUE\_OPERATORS & 0.0007 & Little Impact \\
        NUM\_OPERATORS & 0.0003 & Little Impact \\
        BRANCH\_COUNT & 0.0002 & Little Impact \\
        HALSTEAD\_VOLUME & 0.0001 & Little Impact \\
        HALSTEAD\_CONTENT & 0.0001 & Little Impact \\
        HALSTEAD\_DIFFICULTY & 0.0001 & Little Impact \\
        \bottomrule
    \end{tabular*}
\end{table*}


\subsection{Logistic Regression on GHPR}
Multiple evaluation metrics were used when training and testing the FCNN model; confusion matrices and an accuracy score are calculated using the test set and are used to determine if the models are effectively classifying the datapoints. To observe sparsity in our logistic regression models, we view the absolute theta value that corresponds to each feature; the larger that theta value, the higher importance we assign to the corresponding feature, if a feature has a theta $<$ 0.001, we assume little to no impact is caused by that feature.

This model had an accuracy of 71.68\% with the following confusion matrix and Feature Importance Chart

\begin{table}[h]
\centering
 
\begin{tabular}{|m{1cm}| m{1.5cm} |m{2.5cm}|}
\hline
\textbf{Metric Name} & \textbf{Weight Magnitude} & \textbf{Impact} \\ \hline
nosi & 2.1487 & $\downarrow$ Probability \\ \hline
dit & 0.5788 & $\downarrow$ Probability \\ \hline
cbo & 0.5759 & $\uparrow$ Probability \\ \hline
rfc & 0.3825 & $\downarrow$ Probability \\ \hline
loc & 0.2674 & $\uparrow$ Probability \\ \hline

\end{tabular}
\caption{Feature Importance for Defect Prediction, view model.ipynb for full table}
\label{tab:feature_importance}
\end{table}

\includegraphics[scale = 0.25]{LR GHRP Confusion Matrix.png}

\includegraphics[scale = 0.25]{LR GHRP Feature Importance.png}
\subsection{FCNN}
Multiple evaluation metrics were used when training and testing the FCNN model; confusion matrices and an accuracy score are calculated using the test set and are used to determine if the models are effectively classifying the datapoints. To observe the sparsity of our FCNN, each feature's importance is represented by its contribution to the total weights of the first layer.
\begin{itemize}
    \item $w_{j,i}$ represents the \textit{i}th weight of the \textit{j}th input
    \item $s_j$ represents the sum of the weights of the \textit{j}th input
\end{itemize}

$$s_j = \sum_{i=1}^n w_{j,i}$$
$$ W = \sum_{i=1,j=1}^{n,m}w_{j,i}$$
$$ns_j = \frac{s_j}{W}$$

\textbf{The larger the $ns_j$ value the more impactful we determine the \textit{j}th feature}

The FCNN had an accuracy of 75.64\% with the following confusion matrix and Feature Importance Chart

\includegraphics[scale = 0.3]{FCNN Confusion Matrix.png}

\includegraphics[scale = 0.3]{FCNN Feature Importance.png}


\section{Analysis}

\subsection{PROMISE Dataset}

Using the modified logistic regression model, 6 important features were identified by the model, whereas the remaining features had zeroed or nearly-zeroed coefficients suggesting a low relative importance in defect identification. The 6 identified features (in order of importance) are: 



\vspace{\baselineskip}\noindent \textbf{ NUM\_\allowbreak UNIQUE\_\allowbreak OPERANDS}
  
\noindent The number of unique operands in a software module corresponds to the size of the 'vocabulary' used to define it. A module with many distinct variables may be more defect prone because a developer has to keep track of a large number of variables at once, which may increase the likelihood of a logic error. State and complexity also go hand in hand, so it's logical that a large number of state variables may be correlated with defects. 

\vspace{\baselineskip}\noindent    \textbf{LOC\_\allowbreak EXECUTABLE}
  
\noindent This result suggests that larger programs are more defect prone. This is an intuitive result since logically, more code means that there are more places for a defect to exist, thus increasing the probability of module defectiveness. It is significant that such a simple metric is so powerful at predicting this result, as it reinforces the benefits of small, single purpose modules as taught in software engineering best practices. 

\vspace{\baselineskip}\noindent    \textbf{LOC\_\allowbreak COMMENTS}
  
\noindent This result is unexpected, as logically comments can never produce a bug in a module and are stressed as best practice to write well-commented code. However, it is conceivable that more 'difficult' code is commented heavier particularly in cases where the logic may be nontrivial, thus simpler and understanable code that may be less defect-prone posess fewer comments as a result. This may also be a side effect of the fact that larger codebases have more comments, and larger codebases are more likely to contain a defect.
\vspace{\baselineskip}\noindent  \textbf{DESIGN\_\allowbreak COMPLEXITY}
  
\noindent  This metric is a variant of Cyclomatic Complexity, focusing specifically on branches that calll other modules. This result seems to suggest that 'high traffic' modules are more likely to possess a bug. 

\vspace{\baselineskip}\noindent    \textbf{CYCLOMATIC\_\allowbreak COMPLEXITY}
  
\noindent Since Cyclomatic Complexity is a measure of branching / independent paths through the code, it may be the case that since high path count modules are less prone to being exhaustively tested, this increases the chance of a defect going unnoticed during testing. This seems to suggest that more complex branching structures give rise to uncovered edge-cases, which is an intuitive result for a programmer. 

\vspace{\baselineskip}\noindent    \textbf{HALSTEAD\_ERROR\_EST}
  
\noindent Halstead Error is the lowest identified metric, and is an aggregate measure of seveal Halstead metrics. It makes sense that this feature holds some predictive power, as it is engineered to specifically be an estimate for defects. 




\subsection{GHPR Dataset}

\textbf{NOSI}

Number of static invocations is by far the most impactful metric, and it is shown to have a negative correlation to the probability of a defect being in the code sample; I.e. the more static method calls the less likely there is to be a defect. This could be because code with many static method calls tends to have many static methods defined (i.e it is structured) or is using predefined methods from dependencies (which test to be tested more than code written for an individual).
\\

\noindent\textbf{DIT}

Depth inheritance tree is the second most impactful metric, and it has a negative correlation to the probability of a defect occurring; i.e the more "fathers" classes have the less likely defects occur. This could be because more abstract code is more flexible, leading to less errors. Of course this probably has it's limits, abstracting classes for the sake of abstracting isn't going to improve your code.
\\

\noindent\textbf{CBO}

Coupling between objects is the third most impactful metric with a positive correlation to defect occurrence; i.e. the more dependencies classes have the more likely a defect will occur. There is just more factors to consider when implementing classes with more dependencies, so people are more likely to make mistakes.
\\

\noindent\textbf{RFC}

Response for a class is the forth/fifth most impactful metric with a negative correlation to defect occurrence; i.e. the more unique method invocations in classes the less likely a defect will occur. More unique method invocation may implies classes are more carefully segmented between methods.
\\

\noindent\textbf{LOC}

Lines of code is the forth/fifth most impactful metric with a positive correlation to defect occurrence. As there are more chance to make mistakes as one writes more code of course defects scale with lines of code. One can also argue that DIT, CBO, and RFC increase as LOC increases so there may be some redundancy in this metric.
\\

\noindent\textbf{Conclusions}


Based off the GHPR dataset one can guess that first and foremost using pre-tested dependencies is much more effective than writing your own (This is likely why NOSI has such a large impact on the model). Additionally, having segmented and abstract code can reduce defects, though the more code one writes the more likely one is to make a mistake.

The sparsity of the trained models across both datasets validates Goodhart's law in a quantitative way. The fact that many traditional static metrics had little impact in predicting defects suggests that chasing these metrics may be misguided in the pursuit of eliminating real software defects. 

It is also critical to note that using static metrics as the sole predictor of defect presence is flawed and insufficient to build a 'perfect' defect classifier. While static metrics can often signal an underlying code quality problem that may result in a defect, there are entire classes of defects that are undetectable to this feature set.    

Consider some arbitrary operation on a vector or array. This might be calculating a sum, iterating over items, or a common string operation, like \texttt{strcpy}. A correct version may look like:

\vspace{\baselineskip}
\textbf{Correct Code: }

\begin{verbatim}
  for (int i = 0; i < size; i+=1){
    do_something(v[i]); 
  }
\end{verbatim}

Suppose the programmer makes a logic error, and incorrectly assummes that the first index in an array is 1: 

\vspace{\baselineskip}
\textbf{Defective Code: }

\begin{verbatim}
  for (int i = 1; i < size; i+=1){
    do_something(v[i]); 
  }
\end{verbatim}

However, from the perspective of our static metrics, these code blocks are \textbf{identical}. They posess the same branching structure, same number of operands, and have the same McCabe and Halstead metrics, despite the clear semantic error. Thus, we can show that it's impossible to perfectly characterize a module from these metrics. Models that depend on static metrics alone posess a fundamental information gap preventing a 'perfect' classifier from existing. This also explains the poor upper bound on accuracy obtained by classifiers of this type, as there may always be undetectable defects present in the data. In the context of the \textbf{Bias-Variance tradeoff}, our model possesses a bias problem, as no optimal parameter can resolve these gaps in information. As such, these results are much more meaningful through the lens of feature selection rather than overall accuracy. 


In conclusion, these results suggest that focusing on exhaustively satisfying static code checks as a means of increasing software quality may be ill-advised. If anything, a focus should be placed on a few high-impact metrics that monitor complexity, state, and coupling. Furthermore, the selected features suggest that the size of a module or project is a major driver of defects, which reflects common software engineering design patterns that promote small, modular, and single-responsibility artifacts. 








\begin{thebibliography}{20}

        \bibitem{GHPR}
        \textit{GHPR Dataset}. 
        2021. 
        \url{https://github.com/feiwww/GHPR_dataset} 
        (Accessed: 2025-11-29).

        \bibitem{PROMISE}
        \textit{PROMISE Dataset}.
        2016.
        \url{https://github.com/klainfo/NASADefectDataset}
        (Accessed: 2025-11-25)


        \bibitem{Data Quality}
        \textit{Data Quality: Some Comments on the NASA Software Defect Datasets}
        2013.
        \url{https://ieeexplore.ieee.org/document/6464273}
        (Accessed: 2025-12-10)

        \bibitem{FPPSE}
        \textit{A Systematic Literature Review on Fault Prediction Performance in Software Engineering}
        2012.
        \url{https://ieeexplore.ieee.org/document/6035727}
        (Accessed: 2025-12-10)
\end{thebibliography}

% Bibliography entries for the entire Anthology, followed by custom entries


% Custom bibliography entries only


% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
