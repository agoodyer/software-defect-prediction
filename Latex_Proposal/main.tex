\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{tabularx}
\usepackage{caption}
\usepackage{float}
\usepackage{hyperref}
\usepackage{soul,xcolor}
\usepackage{cancel}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Software Defect Prediction: \\ \large \normalfont An exploration of Model Interpretability in Software Defect Analysis }



\author{Aidan Goodyer, Mason Azzopardi \\
  \texttt{\{goodyera,azzoparm\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}
\setstcolor{red}

\section{Introduction}


In Software Engineering, static code metrics are often used as targets to define a 'quality' gate for a given module or piece of code.  
However, investing time into ensuring these metrics are met is painstaking, and many seem to be poor measures of the module's correctness. 

\noindent A well-known critique of metric-driven evaluation comes from economist Charles Goodhart, whose law states that: 

\begin{quote}
"When a measure becomes a target, it ceases to be a good measure". 
\end{quote}

  \noindent We seek to investigate the legitimacy of this statement as it pertains to static code metrics. Rather than aiming to create the best possible classifier for software defects, we instead aim to answer the question:

\begin{quote}
  "Which static software metrics \textit{actually} matter"?
\end{quote}


  \noindent To investigate this question, we adopt an approach centered on feature selection and model interpretability. 
  Specifically, we employ logistic regression in conjunction with L1 regularization to favour sparsity in the learned parameter, allowing us to derive an understanding of which metrics contribute meaningfully to defect prediction. 
  By favouring an interpretable sparse model, we aim to provide analytical insight into the value of these metrics.

\section{Related Work}

Here, talk about the related work you encountered for your approach. Cite at least 5 references. Refer to item 2. No one has done exactly your task? Write about the most similar thing you can find. This should be around 0.25-0.5 pages.

\section{Dataset}


The NASA Promise Dataset is a collection of roughly 10,000 examples in software defect analysis. These examples consist of static code measures collected from NASA's Metrics Data Program (MDP). Data was collected from real software modules used by various NASA projects over a span of several decades. Data is partitioned into 13 different subsets according to the separate software project source and derived measures, which vary between sets. The attributes are primarily Halstead, McCabe, and Lines of Code related metrics.  

\noindent The original PROMISE repository being no longer accessible, thereforew we opted to use a GitHub mirror of the original dataset. It can be located \underline{\href{https://github.com/klainfo/NASADefectDataset}{here.}}

\subsection{Preprocessing}

The original PROMISE dataset ($D'$) contains significant flaws including duplicate entries, empty modules, and inconsistencies that may pose challenges in training an effective classifier. As such, we opt to start from a pre-cleaned version of the dataset ($D''$) to reduce manual cleaning and deduplication efforts. $D''$ is frequently used as a common baseline across several of the referenced papers. To preprocess the dataset, we convert the source \verb|.arff| files to  \verb|.csv| using a minimal python script.
  
Since the numerical ranges vary extremely for individual input features, we utilize z-score normalization to standardize the deign matrix to have a mean of 0 and a standard deviation of 1 using the formula: 

$$ \phi(x) = \frac{x-\mu}{\sigma}$$

where:

$$ \mu = \frac{1}{n} \sum_{i=1}^n x_i \hspace{5mm} \sigma = \sqrt{ \frac{1}{n}\sum_{i=1}^n (x_i-\mu)^2} $$

Defining a consistent scale is particularly important for the purpose of comparing feature importance, as this ensures that features cannot be scaled by an arbitrary constant to affect the magnitude of its learned weight. 



\section{Features}

The dataset is comprised of 21 static code metrics along with a binary Y/N label indicating the presence of defects. We plan to use all of the included metrics in our model, except for 'LOC\_BLANK', 'LOC\_TOTAL', and 'LOC\_AND\_COMMENT'. The rationale for dropping these metrics is due to their extreme collineariy, as these metrics are simply proxies for Lines of Code in every case. 

We can formulate the model as a binary classification problem of the form: 

\begin{itemize}
  \item \textbf{Input Feature (x):} The 21 static code metrics.
  \item \textbf{Label (y):} Binary Defect / No-Defect Label.
\end{itemize}

See Table \underline{\ref{tab:metrics}} for a description of each of the 21 metrics. 


\begin{table}[htbp]
    \centering
    \footnotesize 
    \begin{tabularx}{\columnwidth}{@{} l X @{}}
        \toprule
        \textbf{Metric} & \textbf{Description} \\
        \midrule
        1. loc (v) & Line count of code \\
        2. v (g) & Cyclomatic complexity \\
        3. ev (g) & Essential complexity \\
        4. iv (g) & Design complexity \\
        5. \textcolor{red}{\st{loCode}} & Line count \\
        6. loComment & Count of lines of comments \\
        7. \textcolor{red}{\st{loBlank}} & Count of blank lines \\
        8. \textcolor{red}{\st{loCodeAndComment}} & Count of code and comment lines \\
        9. uniq\_Op & Unique operators \\
        10. uniq\_Opnd & Unique operands \\
        11. total\_Op & Total operators \\
        12. total\_Opnd & Total operands \\
        13. branchCount & Branch count of the flow graphs \\
        14. n & Total operators + operands \\
        15. v & Volume \\
        16. l & Program length \\
        17. d & Difficulty \\
        18. i & Intelligence \\
        19. e & Effort \\
        20. b & Estimate of the effort \\
        21. t & Time estimator \\
        22. Defect & True/False \\
        \bottomrule
    \end{tabularx}
        \caption{The summary of code metrics (red unused)\label{tab:metrics}}
\end{table}

\section{Implementation}

\subsection{Logistic Regression Model}

Our implementation utilizes a Logistic Regression model trained using gradient descent. The model is modified to use L1 regularization and an asymmetric loss function to meet the needs of the defect prediction task. 

The probability of the prensence of a defect in a module, $p(i)$ is calculated using sigmoid activation:

$$
p(i) = \sigma\!\left( \theta^\top x^{(i)} + \theta_0 \right)
= \frac{1}{1 + e^{-\left( \theta^\top x^{(i)} + \theta_0 \right)}}
$$

Two key changes were made to favour interpretability and correct classification of the minority class. 

First, we modify the logistic regression to be asymmetric. When true defects are misclassified, we apply an asymmetric penalty to heavily discourage this type of mistake in the model. Since the consequences of missing a software defect are far more severe than investigating a few 'false alarm' cases, skewing the penalty favors a model that minimizes these critical mistakes. 

We define the weighted error term $e^{(i)}$ be: 


$$e^{(i)} = \begin{cases} 4(p^{(i)} - 1) & \text{if } y^{(i)} = 1 \\ p^{(i)} - 0 & \text{if } y^{(i)} = 0 \end{cases}$$


The second modification we make to standard logistic regression is L1 regularization. This choice encourages sparsity in the learned weights, helping to drive irrelevant metrics to zero within the model. This is ideal for our feature selection goals as it drives the model to surface the most 'useful' features for prediction. 

With this modification, the gradients of our loss function become: 

$$\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^{(t)}, \theta_0^{(t)}) = \frac{1}{n} \sum_{i=1}^{n} e^{(i)} \mathbf{x}^{(i)} + \lambda \cdot \text{sign}(\boldsymbol{\theta}^{(t)})$$

$$\nabla_{\theta_0}J(\boldsymbol{\theta}^{(t)}, \theta_0^{(t)}) = \frac{1}{n} \sum_{i=1}^{n} e^{(i)} $$


Using the definition of the $L_1$ norm:
$$\lVert x \rVert _1 = \sum_{i=1}^{n} |x_i|$$

\noindent The gradient of the $\lambda$ term becomes $\text{sign}(\boldsymbol{\theta}^{(t)})$ since $\frac{d}{dx} \left|x\right| = \text{sign}(x) $ 


Lastly, we use the standard gradient descent update rule (with $\eta = 0.1$): 


$$\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \eta \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^{(t)}, \theta_0^{(t)})$$

$$\theta_0^{(t+1)} = \theta_0^{(t)} - \eta \nabla_{\theta_0}J(\boldsymbol{\theta}^{(t)}, \theta_0^{(t)})$$

\noindent Our model is trained for 1000 iterations as experimentally convergence occurs within a safe margin of this bound. 

\section{Results and Evaluation}


\noindent \textbf{Confusion Matrix: Training Set}
\[
\begin{array}{c|cc}
 & \hat{y}=0 & \hat{y}=1 \\
\hline
y=0 & 3186~(\mathrm{TN}) & 1700~(\mathrm{FP}) \\
y=1 & 485~(\mathrm{FN}) & 805~(\mathrm{TP})
\end{array}
\]


\noindent \textbf{Confusion Matrix: Test Set}
\[
\begin{array}{c|cc}
 & \hat{y}=0 & \hat{y}=1 \\
\hline
y=0 & 787~(\mathrm{TN}) & 435~(\mathrm{FP}) \\
y=1 & 110~(\mathrm{FN}) & 212~(\mathrm{TP})
\end{array}
\]

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{../log-regression-figures/accuracy.png}
  \caption{Accuracy results for the Logistic Regression Model on the PROMISE dataset}
  \label{fig:accuracy-log-promise}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{../log-regression-figures/features.png}
  \caption{Feature Importance for the Logistic Regression Model on the PROMISE dataset}
  \label{fig:accuracy-log-promise}
\end{figure}




\section{Template Notes}

You can remove this section or comment it out, as it only contains instructions for how to use this template. You may use subsections in your document as you find appropriate.

\subsection{Tables and figures}

See Table~\ref{citation-guide} for an example of a table and its caption.
See Figure~\ref{fig:experiments} for an example of a figure and its caption.


\begin{figure}[t]
  \includegraphics[width=\columnwidth]{example-image-golden}
  \caption{A figure with a caption that runs for more than one line.
    Example image is usually available through the \texttt{mwe} package
    without even mentioning it in the preamble.}
  \label{fig:experiments}
\end{figure}

\begin{figure*}[t]
  \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
  \includegraphics[width=0.48\linewidth]{example-image-b}
  \caption {A minimal working example to demonstrate how to place
    two images side-by-side.}
\end{figure*}


\subsection{Citations}

\begin{table*}
  \centering
  \begin{tabular}{lll}
    \hline
    \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
    \hline
    \citep{Gusfield:97}       & \verb|\citep|           &                           \\
    \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
    \citet{Gusfield:97}       & \verb|\citet|           &                           \\
    \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
    \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
    \hline
  \end{tabular}
  \caption{\label{citation-guide}
    Citation commands supported by the style file.
  }
\end{table*}

Table~\ref{citation-guide} shows the syntax supported by the style files.
We encourage you to use the natbib styles.
You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

\subsection{References}

\nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

Many websites where you can find academic papers also allow you to export a bib file for citation or bib formatted entry. Copy this into the \texttt{custom.bib} and you will be able to cite the paper in the \LaTeX{}. You can remove the example entries.

\subsection{Equations}

An example equation is shown below:
\begin{equation}
  \label{eq:example}
  A = \pi r^2
\end{equation}

Labels for equation numbers, sections, subsections, figures and tables
are all defined with the \verb|\label{label}| command and cross references
to them are made with the \verb|\ref{label}| command.
This an example cross-reference to Equation~\ref{eq:example}. You can also write equations inline, like this: $A=\pi r^2$.


% \section*{Limitations}

\section*{Team Contributions}

Write in this section a few sentences describing the contributions of each team member. What did each member work on? Refer to item 7.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
