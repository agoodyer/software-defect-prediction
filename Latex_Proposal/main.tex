\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{tabularx}
\usepackage{caption}
\usepackage{float}
\usepackage{hyperref}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Software Defect Prediction: \\ \large \normalfont An exploration of Model Interpretability in Software Defect Analysis }



\author{Aidan Goodyer, Mason Azzopardi \\
  \texttt{\{goodyera,azzoparm\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}


In Software Engineering, static code metrics are often used as targets to define a 'quality' gate for a given module or piece of code.  
However, investing time into ensuring these metrics are met is painstaking, and many seem to be poor measures of the module's correctness. 

\noindent A well-known critique of metric-driven evaluation comes from economist Charles Goodhart, whose law states that: 

\begin{quote}
"When a measure becomes a target, it ceases to be a good measure". 
\end{quote}

  \noindent We seek to investigate the legitimacy of this statement as it pertains to static code metrics. Rather than aiming to create the best possible classifier for software defects, we instead aim to answer the question:

\begin{quote}
  "Which static software metrics \textit{actually} matter"?
\end{quote}


  \noindent To investigate this question, we adopt an approach centered on feature selection and model interpretability. 
  Specifically, we employ logistic regression in conjunction with L1 regularization to favour sparsity in the learned parameter, allowing us to derive an understanding of which metrics contribute meaningfully to defect prediction. 
  By favouring an interpretable sparse model, we aim to provide analytical insight into the value of these metrics.



\section{Related Work}

Here, talk about the related work you encountered for your approach. Cite at least 5 references. Refer to item 2. No one has done exactly your task? Write about the most similar thing you can find. This should be around 0.25-0.5 pages.

\section{Dataset}


The NASA Promise Dataset is a collection of roughly 10,000 examples in software defect analysis. These examples consist of static code measures collected from NASA's Metrics Data Program (MDP). Data was collected from real software modules used by various NASA projects over a span of several decades. Data is partitioned into 13 different subsets according to the separate software project source and derived measures, which vary between sets. The attributes are primarily Halstead, McCabe, and Lines of Code related metrics.  

\noindent The original PROMISE repository being no longer accessible, thereforew we opted to use a GitHub mirror of the original dataset. It can be located \underline{\href{https://github.com/klainfo/NASADefectDataset}{here.}}

The second dataset used is the baseline from the GHRP dataset. It was created using the CK code metrics tool on the 6052 instances GHRP dataset. It contains the SHA to the sample code instance, 21 static code metrics, and a label (1/0) informing whether there is a defect or not for every instance.

\subsection{Preprocessing}

The original \textbf{PROMISE} dataset ($D'$) contains significant flaws including duplicate entries, empty modules, and inconsistencies that may pose challenges in training an effective classifier. As such, we opt to start from a pre-cleaned version of the dataset ($D''$) to reduce manual cleaning and deduplication efforts. $D''$ is frequently used as a common baseline across several of the referenced papers. The extent of our preprocessing is solely in converting the source \verb|.arff| files to  \verb|.csv| using a minimal python script.

The \textbf{GHPR} dataset required little preprocessing. 
\begin{enumerate}
  \item SHA column removed
  \item label seperated from static code metrics and assigned as the labels and input values respectively
  \item the labels and input values were split into an 80/20 train/test split
  \item input values are standardized
  \item \textbf{FCNN ONLY} input values transformed into tensor values
\end{enumerate}





\section{Features}

The \textbf{PROMISE} dataset is comprised of 21 static code metrics along with a binary Y/N label indicating the presence of defects. We plan to use all of the included metrics in our model.

We can formulate the model as a binary classification problem of the form: 

\begin{itemize}
  \item \textbf{Input Feature (x):} The 21 static code metrics.
  \item \textbf{Label (y):} Binary Defect / No-Defect Label.
\end{itemize}

See Table \underline{\ref{tab:PROMISE metrics}} for a description of each of the 21 metrics. 


\begin{table}[htbp]
    \centering
    \footnotesize 
    \begin{tabularx}{\columnwidth}{@{} l X @{}}
        \toprule
        \textbf{Metric} & \textbf{Description} \\
        \midrule
        1. loc (v) & Line count of code \\
        2. v (g) & Cyclomatic complexity \\
        3. ev (g) & Essential complexity \\
        4. iv (g) & Design complexity \\
        5. loCode & Line count \\
        6. loComment & Count of lines of comments \\
        7. loBlank & Count of blank lines \\
        8. loCodeAndComment & Count of code and comment lines \\
        9. uniq\_Op & Unique operators \\
        10. uniq\_Opnd & Unique operands \\
        11. total\_Op & Total operators \\
        12. total\_Opnd & Total operands \\
        13. branchCount & Branch count of the flow graphs \\
        14. n & Total operators + operands \\
        15. v & Volume \\
        16. l & Program length \\
        17. d & Difficulty \\
        18. i & Intelligence \\
        19. e & Effort \\
        20. b & Estimate of the effort \\
        21. t & Time estimator \\
        22. Defect & True/False \\
        \bottomrule
    \end{tabularx}
        \caption{The summary of PROMISE code metrics\label{tab:PROMISE metrics}}
\end{table}
The \textbf{GHPR} dataset is comprised of 21 static code metrics along with a binary Y/N label indicating the presence of defects. We plan to use all of the included metrics in our model.
\begin{table}[htbp]
    \centering
    \footnotesize 
    \begin{tabularx}{\columnwidth}{@{} l X @{}}
        \toprule
        \textbf{Metric} & \textbf{Description} \\
        \midrule
        1. CBO (v) & Coupling between objects. Counts the number of dependencies a class has. \\
        2. WMC (g) & 	Weight Method Class or McCabe's complexity. It counts the number of branch instructions in a class.\\
        3. DIT (g) & Depth Inheritance Tree. It counts the number of "fathers" a class has. All classes have DIT at least 1 (everyone inherits java.lang.Object). \\
        4. rfc (g) & Response for a Class. Counts the number of unique method invocations in a class. \\
        5. lcom & Lack of Cohesion of Methods. Calculates LCOM metric. \\
        6. totalMethods & Counts the number of methods. \\
        7. totalFields & Counts the number of fields. \\
        8. NOSI & Number of static invocations. Counts the number of invocations to static methods. \\
        9. LOC & Lines of code. It counts the lines of count, ignoring empty lines. \\
        10. returnQty & Quantity of returns. The number of return instructions. \\
        11. loopQty & Quantity of loops. The number of loops (i.e., for, while, do while, enhanced for). \\
        12. comparisonsQty & Quantity of comparisons. The number of comparisons (i.e., == and !=). \\
        13. tryCatchQty & Quantity of try/catches. The number of try/catches. \\
        14. parenthesizedExpsQty & Quantity of parenthesized expressions. The number of expressions inside parenthesis. \\
        15. stringLiteralsQty & String literals. The number of string literals (e.g., "John Doe"). \\
        16. numbersQty & Quantity of Number. The number of numbers (i.e., int, long, double, float) literals. \\
        17. assignmentsQty & Quantity of Variables. Number of declared variables. \\
        18. mathOperationsQty & Quantity of Math Operations: The number of math operations (times, divide, remainder, plus, minus, left shit, right shift). \\
        19. variablesQty & Quantity of Variables. Number of declared variables. \\
        20. maxNestedBlocks & Max nested blocks. The highest number of blocks nested together. \\
        21. uniqueWordsQty & Number of unique words. Number of unique words in the source code. \\
        \bottomrule
    \end{tabularx}
        \caption{The summary of GHPR code metrics\label{tab:GHRP metrics}}
\end{table}
We can formulate the model as a binary classification problem of the form: 

\begin{itemize}
  \item \textbf{Input Feature (x):} The 21 static code metrics.
  \item \textbf{Label (y):} Binary Defect / No-Defect Label.
\end{itemize}

See Table \underline{\ref{tab:metrics}} for a description of each of the 21 metrics. 

\section{Implementation}

\subsection{Fully Connected Neural Network (FCNN)}
The FCNN model was constructed in PyTorch with the following architecture:
\begin{itemize}
    \item \textbf{Input Layer:} Accepts 21 input features
    \item \textbf{Hidden Layer 1:} 32 neurons, ReLU activation, 50\% Dropout.
    \item \textbf{Hidden Layer 2:} 16 neurons, ReLU activation, 50\% Dropout.
    \item \textbf{Output Layer:} Single neuron with sigmoid activation for classification
\end{itemize}
The FCNN model was trained with stochastic gradient descent (SGD) using a Binary Cross-Entropy Loss as its loss function.

BCE loss:
$$-{(y\log(p) + (1 - y)\log(1 - p))}$$

\section{Results and Evaluation}
\subsection{Logistic Regression on GHPR}
Multiple evaluation metrics were used when training and testing the FCNN model; confusion matrices and an accuracy score are calculated using the test set and are used to determine if the models are effectively classifying the datapoints. To observe sparsity in our logistic regression models, we view the absolute theta value that corresponds to each feature; the larger that theta value, the higher importance we assign to the corresponding feature, if a feature has a theta $<$ 0.001, we assume little to no impact is caused by that feature.

This model had an accuracy of 71.68\% with the following confusion matrix and Feature Importance Chart

\begin{table}[h]
\centering
 
\begin{tabular}{|m{1cm}| m{1.5cm} |m{2.5cm}|}
\hline
\textbf{Metric Name} & \textbf{Weight Magnitude} & \textbf{Impact} \\ \hline
nosi & 2.1487 & $\downarrow$ Probability \\ \hline
dit & 0.5788 & $\downarrow$ Probability \\ \hline
cbo & 0.5759 & $\uparrow$ Probability \\ \hline
rfc & 0.3825 & $\downarrow$ Probability \\ \hline
loc & 0.2674 & $\uparrow$ Probability \\ \hline

\end{tabular}
\caption{Feature Importance for Defect Prediction, view model.ipynb for full table}
\label{tab:feature_importance}
\end{table}

\includegraphics[scale = 0.25]{LR GHRP Confusion Matrix.png}

\includegraphics[scale = 0.25]{LR GHRP Feature Importance.png}
\subsection{FCNN}
Multiple evaluation metrics were used when training and testing the FCNN model; confusion matrices and an accuracy score are calculated using the test set and are used to determine if the models are effectively classifying the datapoints. To observe the sparsity of our FCNN, each feature's importance is represented by its contribution to the total weights of the first layer.
\begin{itemize}
    \item $w_{j,i}$ represents the \textit{i}th weight of the \textit{j}th input
    \item $s_j$ represents the sum of the weights of the \textit{j}th input
\end{itemize}

$$s_j = \sum_{i=1}^n w_{j,i}$$
$$ W = \sum_{i=1,j=1}^{n,m}w_{j,i}$$
$$ns_j = \frac{s_j}{W}$$

\textbf{The larger the $ns_j$ value the more impactful we determine the \textit{j}th feature}

The FCNN had an accuracy of 75.64\% with the following confusion matrix and Feature Importance Chart

\includegraphics[scale = 0.3]{FCNN Confusion Matrix.png}

\includegraphics[scale = 0.3]{FCNN Feature Importance.png}


\section{Analysis}
\subsection{GHPR Dataset}

\textbf{NOSI}

Number of static invocations is by far the most impactful metric, and it is shown to have a negative correlation to the probability of a defect being in the code sample; I.e. the more static method calls the less likely there is to be a defect. This could be because code with many static method calls tends to have many static methods defined (i.e it is structured) or is using predefined methods from dependencies (which test to be tested more than code written for an individual).
\\

\noindent\textbf{DIT}

Depth inheritance tree is the second most impactful metric, and it has a negative correlation to the probability of a defect occurring; i.e the more "fathers" classes have the less likely defects occur. This could be because more abstract code is more flexible, leading to less errors. Of course this probably has it's limits, abstracting classes for the sake of abstracting isn't going to improve your code.
\\

\noindent\textbf{CBO}

Coupling between objects is the third most impactful metric with a positive correlation to defect occurrence; i.e. the more dependencies classes have the more likely a defect will occur. There is just more factors to consider when implementing classes with more dependencies, so people are more likely to make mistakes.
\\

\noindent\textbf{RFC}

Response for a class is the forth/fifth most impactful metric with a negative correlation to defect occurrence; i.e. the more unique method invocations in classes the less likely a defect will occur. More unique method invocation may implies classes are more carefully segmented between methods.
\\

\noindent\textbf{LOC}

Lines of code is the forth/fifth most impactful metric with a positive correlation to defect occurrence. As there are more chance to make mistakes as one writes more code of course defects scale with lines of code. One can also argue that DIT, CBO, and RFC increase as LOC increases so there may be some redundancy in this metric.
\\

\noindent\textbf{Conclusions}

Based off the GHPR dataset one can guess that first and foremost using pre-tested dependencies is much more effective than writing your own (This is likely why NOSI has such a large impact on the model). Additionally, having segmented and abstract code can reduce defects, though the more code one writes the more likely one is to make a mistake.
\end{document}


\section{Template Notes}

You can remove this section or comment it out, as it only contains instructions for how to use this template. You may use subsections in your document as you find appropriate.


\section*{Team Contributions}

Write in this section a few sentences describing the contributions of each team member. What did each member work on? Refer to item 7.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
